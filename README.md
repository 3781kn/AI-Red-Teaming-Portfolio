# AI-Red-Teaming-Portfolio
Advanced LLM Red Teaming & Adversarial Research Portfolio. A comprehensive repository documenting jailbreaks, prompt injections, and safety bypass behaviors across frontier models. Work ranges from tactical adversarial interactions to deep-dive analysis of model behavior, institutional failure modes, and socio-technical risk mitigation. All work is conducted for AI safety research, risk assessment, and responsible disclosure purposes
